{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f481860c",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Load the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b82eb7bf",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "import re\n",
    "# import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "288b167a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Primary libraries\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b441889d",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Secondary libraries\n",
    "from tqdm import tqdm\n",
    "# from scipy import stats\n",
    "import scipy\n",
    "# from glob import glob\n",
    "import os\n",
    "# from matplotlib import dates as mpl_dates\n",
    "# import datetime\n",
    "# from datetime import date\n",
    "# import matplotlib.patches as mpatches\n",
    "# from matplotlib import cm\n",
    "# from colorspacious import cspace_converter\n",
    "# from collections import OrderedDict\n",
    "# import matplotlib.ticker as ticker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea9594a",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Readings for the analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fca48fc",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We are dealing with .gb files with single records each.\n",
    "\n",
    "From (https://warwick.ac.uk/fac/sci/moac/people/students/peter_cock/python/genbank/)-\n",
    "\n",
    "    Depending on the type of GenBank file(s) you are interested in, they will either contain a single record, or multiple records. You can easily determine this by looking at the raw file - each record will start with a LOCUS line, followed by various other header lines, usually a list of features, the sequence data, and ends with a // line (slash slash)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d11e63",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Locations provided by **BioPython** is optimum for python purposes.\n",
    "- start and end provided by it are 1938 and 3075 respectively (0-based indexing and it assumes that end position is not included),\n",
    "- while in our actual file it is 1939 and 3075 (1-based indexing and it assumes that both start and end positions are included).\n",
    "\n",
    "This way we can directly slice seq string using locations provided to obtain the seq for the features of our interest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0533de57",
   "metadata": {
    "hidden": true
   },
   "source": [
    "1. **The DDBJ/ENA/GenBank Feature Table Definition**: \n",
    "    Documentation of features in genbank files. Very good document, must go-through this once.\n",
    "    \n",
    "    Source:\n",
    "    - (https://www.insdc.org/documents/feature_table.html)\n",
    "\n",
    "\n",
    "\n",
    "2. **Locus tag:**\n",
    "    Locus_tags are identifiers that are systematically applied to every gene in a genome.\n",
    "    These tags have become surrogate gene names by the biological community. If two\n",
    "    submitters of two different genomes use the same systematic names to describe two very\n",
    "    different genes in two very different genomes, it can be very confusing. In order to\n",
    "    prevent this from happening INSD has created a registry of locus_tag prefixes.\n",
    "    Submitters of eukaryotic and prokaryotic genomes should register their prefix prior to\n",
    "    submitting their genome. All components of a project (such as multiple chromosomes or\n",
    "    plasmids, etc) should use the same locus_tag prefix. \n",
    "    \n",
    "    Source:\n",
    "    - (https://www.ncbi.nlm.nih.gov/genomes/locustag/Proposal.pdf)\n",
    "    - (https://www.ncbi.nlm.nih.gov/genbank/genomesubmit_annotation/#locus_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87de23a",
   "metadata": {},
   "source": [
    "### Extracting desirable information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0e198c",
   "metadata": {},
   "source": [
    "Useful videos for the analysis done later-\n",
    "1. https://www.youtube.com/watch?v=LdQV3cbUwEE&list=PLe1-kjuYBZ05T9iHV_z60B9mpFt201ND5&index=8\n",
    "2. https://www.youtube.com/watch?v=HP7ThAj_f1E\n",
    "\n",
    "_Both videos are on Youtube @Bioinformatics Coach_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc6b423",
   "metadata": {},
   "source": [
    "KeyError resolution -\n",
    "\n",
    "    gene_name = gene.qualifiers['gene'][0]\n",
    "    gene_name = gene.qualifiers.get('gene',['unavailable'])[0]\n",
    "\n",
    "_Source_: https://bioinformatics.stackexchange.com/questions/15454/keyerror-when-getting-features-from-a-genbank-file-with-biopython-with-some-acce/15456#15456"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7650a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# desirable FLANK length\n",
    "FLANK_LENGTH = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "304066eb",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def genbank_file_reader(file_name):\n",
    "    \"\"\"Takes in genbank file name in the folder ./genbank_files/;\n",
    "    Outputs the dataframe for that genbank file\n",
    "    \"\"\"\n",
    "    gb_record = SeqIO.read(open(f\"./genbank_files/{file_name}\", 'r'), 'genbank')\n",
    "    print(f'Name {gb_record.name}, {len(gb_record.features)} features')\n",
    "    \n",
    "    data = []\n",
    "\n",
    "    allgenes = (\n",
    "        feature\n",
    "        for feature in gb_record.features\n",
    "        if feature.type == 'gene'\n",
    "    )\n",
    "    \n",
    "    for gene in allgenes:\n",
    "\n",
    "        gene_name = gene.qualifiers.get('gene',['unavailable'])[0]\n",
    "        gene_ID = gene.qualifiers.get('db_xref', ['GeneID:unavailable'])[0][7:]\n",
    "        locus_tag = gene.qualifiers.get('locus_tag', ['unavailable'])[0]\n",
    "\n",
    "        start_pos = gene.location.nofuzzy_start\n",
    "        # start_pos = gene.location.nofuzzy_start + 1 # if 1-based indexing required\n",
    "        end_pos = gene.location.nofuzzy_end\n",
    "        strand_sense = gene.strand\n",
    "\n",
    "        gene_seq = gene.extract(gb_record).seq\n",
    "\n",
    "        # For +ve sense strand\n",
    "        if strand_sense == 1:\n",
    "            if len(gb_record.seq[:start_pos]) >= FLANK_LENGTH:\n",
    "                upstream_flank = gb_record.seq[start_pos-FLANK_LENGTH:start_pos]\n",
    "            else:\n",
    "                upstream_flank = gb_record.seq[:start_pos]\n",
    "\n",
    "            if len(gb_record.seq[end_pos:]) >= FLANK_LENGTH:\n",
    "                downstream_flank = gb_record.seq[end_pos:end_pos+FLANK_LENGTH]\n",
    "            else:\n",
    "                downstream_flank = gb_record.seq[end_pos:]\n",
    "        # For -ve sense strand\n",
    "        elif strand_sense == -1:\n",
    "            if len(gb_record.seq[:start_pos]) >= FLANK_LENGTH:\n",
    "                downstream_flank = gb_record.seq[start_pos-FLANK_LENGTH:start_pos].reverse_complement()\n",
    "            else:\n",
    "                downstream_flank = gb_record.seq[:start_pos].reverse_complement()\n",
    "\n",
    "            if len(gb_record.seq[end_pos:]) >= FLANK_LENGTH:\n",
    "                upstream_flank = gb_record.seq[end_pos:end_pos+FLANK_LENGTH].reverse_complement()\n",
    "            else:\n",
    "                upstream_flank = gb_record.seq[end_pos:].reverse_complement()\n",
    "        \n",
    "        \n",
    "        pattern_length = 4\n",
    "        # Added a FLANK_LENGTH + (pattern_length - 1) bp motif instead of FLANK_LENGTH to handle the 0 count for boundary cases\n",
    "        # For +ve sense strand\n",
    "        if strand_sense == 1:\n",
    "            if len(gb_record.seq[:start_pos]) >= (FLANK_LENGTH+(pattern_length-1)):\n",
    "                upstream_flank_adjusted = gb_record.seq[start_pos-FLANK_LENGTH:(start_pos+(pattern_length-1))]\n",
    "            else:\n",
    "                upstream_flank_adjusted = gb_record.seq[:(start_pos+(pattern_length-1))]\n",
    "\n",
    "            if len(gb_record.seq[end_pos:]) >= (FLANK_LENGTH+(pattern_length-1)):\n",
    "                downstream_flank_adjusted = gb_record.seq[end_pos:(end_pos+FLANK_LENGTH+(pattern_length-1))]\n",
    "            else:\n",
    "                downstream_flank_adjusted = gb_record.seq[end_pos:]\n",
    "        # For -ve sense strand\n",
    "        elif strand_sense == -1:\n",
    "            if len(gb_record.seq[:start_pos]) >= (FLANK_LENGTH+(pattern_length-1)):\n",
    "                downstream_flank_adjusted = gb_record.seq[start_pos-(FLANK_LENGTH+(pattern_length-1)):start_pos].reverse_complement()\n",
    "            else:\n",
    "                downstream_flank_adjusted = gb_record.seq[:start_pos].reverse_complement()\n",
    "\n",
    "            if len(gb_record.seq[end_pos:]) >= (FLANK_LENGTH+(pattern_length-1)):\n",
    "                upstream_flank_adjusted = gb_record.seq[end_pos-(pattern_length-1):end_pos+FLANK_LENGTH].reverse_complement()\n",
    "            else:\n",
    "                upstream_flank_adjusted = gb_record.seq[end_pos-(pattern_length-1):].reverse_complement()                \n",
    "        # Addition ends ---------------------------------------------------------------            \n",
    "                \n",
    "        data.append((locus_tag, gene_ID, gene_name, start_pos, end_pos, strand_sense, \n",
    "                     str(gene_seq), str(upstream_flank), str(downstream_flank), \n",
    "                     str(upstream_flank_adjusted), str(downstream_flank_adjusted)))    \n",
    "\n",
    "\n",
    "    df = pd.DataFrame(data, columns=['locus_tag', \n",
    "                                     'gene_ID', \n",
    "                                     'gene_name', \n",
    "                                     'start_position', \n",
    "                                     'end_position', \n",
    "                                     'strand_sense', \n",
    "                                     'gene_seq', \n",
    "                                     'upstream_flank', \n",
    "                                     'downstream_flank', \n",
    "                                     'upstream_flank_adjusted', \n",
    "                                     'downstream_flank_adjusted'])\n",
    "\n",
    "    df.to_csv('./csv_files_for_promoters/consensus02_'+file_name[:-3]+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ca22c0",
   "metadata": {},
   "source": [
    "How to iterate over a given directory:\n",
    "https://stackoverflow.com/questions/10377998/how-can-i-iterate-over-files-in-a-given-directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "39b2b522",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "escherichia_coli_BW25113.gb\n",
      "Name CP009273, 9462 features\n",
      "bacillus_subtilis.gb\n",
      "Name NC_000964, 9074 features\n",
      "streptococcus_pneumoniae.gb\n",
      "Name NZ_CP020549, 4328 features\n",
      "klebsiella_pneumoniae.gb\n",
      "Name NC_016845, 10894 features\n"
     ]
    }
   ],
   "source": [
    "# To do the flank calculations for all files\n",
    "directory = os.fsencode('./genbank_files/')\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    filename = os.fsdecode(file)\n",
    "    if filename.endswith('.gb'):\n",
    "        print(filename)\n",
    "        genbank_file_reader(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b42d05",
   "metadata": {},
   "source": [
    "### Loading the csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84a6a1d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: -200, 2: -199, 3: -198, 4: -197, 5: -196, 6: -195, 7: -194, 8: -193, 9: -192, 10: -191, 11: -190, 12: -189, 13: -188, 14: -187, 15: -186, 16: -185, 17: -184, 18: -183, 19: -182, 20: -181, 21: -180, 22: -179, 23: -178, 24: -177, 25: -176, 26: -175, 27: -174, 28: -173, 29: -172, 30: -171, 31: -170, 32: -169, 33: -168, 34: -167, 35: -166, 36: -165, 37: -164, 38: -163, 39: -162, 40: -161, 41: -160, 42: -159, 43: -158, 44: -157, 45: -156, 46: -155, 47: -154, 48: -153, 49: -152, 50: -151, 51: -150, 52: -149, 53: -148, 54: -147, 55: -146, 56: -145, 57: -144, 58: -143, 59: -142, 60: -141, 61: -140, 62: -139, 63: -138, 64: -137, 65: -136, 66: -135, 67: -134, 68: -133, 69: -132, 70: -131, 71: -130, 72: -129, 73: -128, 74: -127, 75: -126, 76: -125, 77: -124, 78: -123, 79: -122, 80: -121, 81: -120, 82: -119, 83: -118, 84: -117, 85: -116, 86: -115, 87: -114, 88: -113, 89: -112, 90: -111, 91: -110, 92: -109, 93: -108, 94: -107, 95: -106, 96: -105, 97: -104, 98: -103, 99: -102, 100: -101, 101: -100, 102: -99, 103: -98, 104: -97, 105: -96, 106: -95, 107: -94, 108: -93, 109: -92, 110: -91, 111: -90, 112: -89, 113: -88, 114: -87, 115: -86, 116: -85, 117: -84, 118: -83, 119: -82, 120: -81, 121: -80, 122: -79, 123: -78, 124: -77, 125: -76, 126: -75, 127: -74, 128: -73, 129: -72, 130: -71, 131: -70, 132: -69, 133: -68, 134: -67, 135: -66, 136: -65, 137: -64, 138: -63, 139: -62, 140: -61, 141: -60, 142: -59, 143: -58, 144: -57, 145: -56, 146: -55, 147: -54, 148: -53, 149: -52, 150: -51, 151: -50, 152: -49, 153: -48, 154: -47, 155: -46, 156: -45, 157: -44, 158: -43, 159: -42, 160: -41, 161: -40, 162: -39, 163: -38, 164: -37, 165: -36, 166: -35, 167: -34, 168: -33, 169: -32, 170: -31, 171: -30, 172: -29, 173: -28, 174: -27, 175: -26, 176: -25, 177: -24, 178: -23, 179: -22, 180: -21, 181: -20, 182: -19, 183: -18, 184: -17, 185: -16, 186: -15, 187: -14, 188: -13, 189: -12, 190: -11, 191: -10, 192: -9, 193: -8, 194: -7, 195: -6, 196: -5, 197: -4, 198: -3, 199: -2, 200: -1}\n"
     ]
    }
   ],
   "source": [
    "mapping_dict = {}\n",
    "\n",
    "mapping_values_list = [i for i in range(-FLANK_LENGTH, 0, 1)]\n",
    "mapping_keys_list = [i for i in range(1, (FLANK_LENGTH+1), 1)]\n",
    "\n",
    "for i in range(len(mapping_keys_list)):\n",
    "    mapping_dict[mapping_keys_list[i]] = mapping_values_list[i]\n",
    "    \n",
    "print(mapping_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303bdbf4",
   "metadata": {},
   "source": [
    "How to iterate rows of a dataframe: https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6d1174e",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def motif_analysis(df, file_name):\n",
    "    \"\"\"Input: dataframe loaded from csv file and its file name\n",
    "    Output: dataframes for upstream- and downstream- flank's motif analysis\n",
    "    \"\"\"\n",
    "    upstream_dict = {}\n",
    "    downstream_dict = {}\n",
    "    for key in keys_list:\n",
    "        upstream_dict[key] = 0\n",
    "        downstream_dict[key] = 0\n",
    "\n",
    "    # Resetting indices to make sure indices pair with number of rows\n",
    "    df = df.reset_index()\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        \n",
    "        # not pd.isnull() statement handles the boundary regions, \n",
    "        # i.e., regions with no flanks for which we'll have 'NaN'\n",
    "        if not pd.isnull(row['upstream_flank_adjusted']):\n",
    "            for m in re.finditer(pattern, row['upstream_flank_adjusted']):\n",
    "                upstream_dict[m.start()+1] += 1\n",
    "        if not pd.isnull(row['downstream_flank_adjusted']):\n",
    "            for m in re.finditer(pattern, row['downstream_flank_adjusted']):\n",
    "                downstream_dict[m.start()+1] += 1\n",
    "    \n",
    "    mapped_upstream_dict = {}\n",
    "    for key in upstream_dict:\n",
    "        mapped_upstream_dict[mapping_dict[key]] = upstream_dict[key]    \n",
    "    \n",
    "    df_up = pd.DataFrame.from_dict(mapped_upstream_dict, orient='index')\n",
    "    df_down = pd.DataFrame.from_dict(downstream_dict, orient='index')\n",
    "    df_up.to_csv('./csv_files_for_promoters_consensus_motifs/consensus02_upstream_'+file_name[12:])\n",
    "    df_down.to_csv('./csv_files_for_promoters_consensus_motifs/consensus02_downstream_'+file_name[12:])\n",
    "    del df_up, df_down\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220cab63",
   "metadata": {},
   "source": [
    "**Always remember to update pattern_length using 'find and replace' when you change a pattern**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "29d81559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "consensus01_streptococcus_pneumoniae.csv\n",
      "2157\n",
      "consensus01_escherichia_coli_BW25113.csv\n",
      "4490\n",
      "consensus01_bacillus_subtilis.csv\n",
      "4536\n",
      "consensus01_klebsiella_pneumoniae.csv\n",
      "5404\n"
     ]
    }
   ],
   "source": [
    "keys_list = [i for i in range(1, (FLANK_LENGTH+1))]\n",
    "\n",
    "# # for consensus 01\n",
    "# pattern = re.compile(r'T[AT]AT')\n",
    "# pattern = re.compile(r'TATAAT')\n",
    "\n",
    "# for consensus 02\n",
    "pattern = re.compile(r'TTG[AT]')\n",
    "# pattern = re.compile(r'TTGACA')\n",
    "\n",
    "# To do the flank calculations for all files\n",
    "directory = os.fsencode('./csv_files_for_promoters/')\n",
    "\n",
    "\n",
    "for file in os.listdir(directory):\n",
    "    \n",
    "    filename = os.fsdecode(file)\n",
    "    if filename.startswith('consensus02') and filename.endswith('.csv'):\n",
    "        print(filename)\n",
    "        df_temp = pd.read_csv('./csv_files_for_promoters/'+filename)\n",
    "        print(len(df_temp.index))\n",
    "        motif_analysis(df_temp, filename)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
